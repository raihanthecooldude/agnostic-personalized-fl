{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a3b9db-a176-46e7-b70b-647175150d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9598b4-7ff4-4897-8f2e-fd9e142209ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to generate synthetic data\n",
    "def generate_data(n, m, d, clusters, noise_var):\n",
    "    X = []\n",
    "    y = []\n",
    "    w_true = []\n",
    "    for c in range(clusters):\n",
    "        w_c = np.random.uniform(-5, 5, d)  # Generate weight vector w_c of size d using uniform distribution\n",
    "        w_true.append(w_c)\n",
    "        for i in range(n // clusters):\n",
    "            X_i = np.random.normal(0, 1, (m, d)) # Generate matrix X_i of size (m, d) using normal / gaussian distribution\n",
    "            y_i = X_i @ w_c + np.random.normal(0, noise_var, m)  # Generate vector y_i of size m using the formula: y_i = X_i * wc + ϵ_i\n",
    "            X.append(X_i)\n",
    "            y.append(y_i)\n",
    "    return X, y, w_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73163d6-5da7-49ac-bb39-bc4c89d98541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Personalized Federated Learning (Algorithm 1)\n",
    "def personalized_federated_learning(X, y, learning_rate=0.01, iterations=100, candidate_count=20):\n",
    "    n = len(X)  # Number of data generators\n",
    "    d = X[0].shape[1]  # Number of features per data generator\n",
    "    \n",
    "    #Step 1: Initialize parameters for data generator 1 (w_hat := 0)\n",
    "    w_est = np.zeros(d)  \n",
    "\n",
    "    # Step 2: Iterate for R iterations\n",
    "    for _ in range(iterations):  \n",
    "        \n",
    "        # Step 3: Randomly choose a subset of candidates (C) excluding data generator\n",
    "        candidates = np.random.choice(range(1, n), size=candidate_count, replace=False)  \n",
    "        \n",
    "        best_candidate = None \n",
    "        max_reward = float('-inf')  \n",
    "        \n",
    "        current_loss = np.mean((y[0] - X[0] @ w_est) ** 2)\n",
    "\n",
    "        for i in candidates:\n",
    "            # Step 4: Compute gradient for each candidate i\n",
    "            grad = -X[i].T @ (y[i] - X[i] @ w_est)\n",
    "            w_temp = w_est - learning_rate * grad\n",
    "\n",
    "            # Step 5: Calculate the loss for each candidate\n",
    "            updated_loss = np.mean((y[0] - X[0] @ w_temp) ** 2)\n",
    "\n",
    "            # Step 6: Determine the best candidate based on maximum reward\n",
    "            reward = current_loss - updated_loss\n",
    "\n",
    "            # Determine the best candidate based on maximum reward\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "                best_candidate = i\n",
    "\n",
    "        # Step 7: Update w_est using the best candidate's gradient if found\n",
    "        if best_candidate is not None:\n",
    "            grad = -X[best_candidate].T @ (y[best_candidate] - X[best_candidate] @ w_est)\n",
    "            w_est -= learning_rate * grad\n",
    "\n",
    "    return w_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bebd181f-ecaa-46a3-a591-61f4a8111b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expectation-Maximization\n",
    "def personalized_expectation_maximization(X, y, clusters, iterations=20, learning_rate=0.01, step_size=0.1, epsilon=0.3, beta=0.6, neighbors_per_iteration=3, stability_epsilon=1e-10):\n",
    "    n = len(X)\n",
    "    d = X[0].shape[1]\n",
    "\n",
    "    # Initialize model parameters (Phi) and priors (Pi)\n",
    "    Phi = np.random.randn(clusters, d)  # Model parameters for each cluster\n",
    "    Pi = np.ones((n, clusters)) / clusters  # Uniform initial priors (representing mixture weights)\n",
    "    L_hat = np.zeros((n, clusters))  # Exponential moving average of losses\n",
    "\n",
    "    for t in range(iterations):\n",
    "        # E-step: Compute the posterior probabilities w_ij for each client\n",
    "        W = np.zeros((n, clusters))  # Weight matrix representing collaboration responsibilities\n",
    "        \n",
    "        # Step 1: Clients independently compute their own posteriors using neighbor selection\n",
    "        for i in range(n):\n",
    "            # ε-greedy neighbor selection: balance between exploration and exploitation\n",
    "            available_neighbors = clusters if clusters < neighbors_per_iteration else neighbors_per_iteration\n",
    "            if np.random.rand() < epsilon:\n",
    "                # Exploration: choose random neighbors\n",
    "                neighbors = np.random.choice(clusters, min(available_neighbors, clusters), replace=False)\n",
    "            else:\n",
    "                # Exploitation: choose neighbors with highest priors (most probable collaborators)\n",
    "                neighbors = np.argsort(Pi[i])[-min(available_neighbors, clusters):]\n",
    "\n",
    "            # Compute posterior responsibilities for client i using only selected neighbors\n",
    "            log_probs = np.full(clusters, -np.inf)  # Default to very low log probability for non-neighbors\n",
    "            \n",
    "            for j in neighbors:\n",
    "                # Calculate the current loss and update the exponential moving average\n",
    "                loss = np.sum((y[i] - X[i] @ Phi[j]) ** 2)\n",
    "                L_hat[i, j] = (1 - beta) * L_hat[i, j] + beta * loss  # Update exponential moving average of loss\n",
    "\n",
    "                # Calculate log prior + negative average loss for posterior computation\n",
    "                log_probs[j] = np.log(Pi[i, j] + stability_epsilon) - L_hat[i, j]  # Adding stability_epsilon to prevent log(0)\n",
    "\n",
    "            # Normalize using softmax to get posterior probabilities for selected neighbors\n",
    "            max_log_prob = np.max(log_probs[neighbors])  # Numerical stability for softmax\n",
    "            exp_probs = np.exp(log_probs[neighbors] - max_log_prob)\n",
    "            W[i, neighbors] = exp_probs / np.sum(exp_probs)  # Normalize to get probabilities for selected neighbors\n",
    "\n",
    "        # Sampling latent variables (z_i) from the categorical distribution represented by W\n",
    "        Z = np.zeros((n,), dtype=int)  # Latent assignments for each data point\n",
    "        for i in range(n):\n",
    "            # Sampling z_i for each data point using the computed posterior probabilities over selected neighbors\n",
    "            Z[i] = np.random.choice(clusters, p=W[i, :])\n",
    "\n",
    "        # Step 2: Decentralized parameter updates by direct gradient sharing between clients\n",
    "        new_Phi = np.copy(Phi)\n",
    "\n",
    "        # Each client collects gradients from neighbors and performs independent updates\n",
    "        for i in range(n):\n",
    "            # Neighbor selection for gradient exchange using ε-greedy strategy\n",
    "            available_neighbors = n if n < neighbors_per_iteration else neighbors_per_iteration\n",
    "            if np.random.rand() < epsilon:\n",
    "                neighbors = np.random.choice(range(n), min(available_neighbors, n), replace=False)\n",
    "            else:\n",
    "                neighbors = np.argsort(Pi[i])[-min(available_neighbors, n):]\n",
    "\n",
    "            grad_accumulator = np.zeros(d)\n",
    "            weight_accumulator = 0\n",
    "\n",
    "            # Receive gradients from selected neighbors and aggregate them\n",
    "            for neighbor in neighbors:\n",
    "                cluster = Z[neighbor]\n",
    "                X_assigned = X[neighbor]\n",
    "                Y_assigned = y[neighbor]\n",
    "\n",
    "                # Compute residual and gradient\n",
    "                residual = X_assigned.dot(Phi[cluster]) - Y_assigned\n",
    "                gradient = X_assigned.T.dot(residual)\n",
    "\n",
    "                # Weight gradient by the posterior responsibility\n",
    "                weight = W[neighbor, cluster]\n",
    "                grad_accumulator += weight * gradient\n",
    "                weight_accumulator += weight * X_assigned.shape[0]\n",
    "\n",
    "            # Independent parameter update by client i for cluster Z[i]\n",
    "            cluster = Z[i]\n",
    "            if weight_accumulator > 0:\n",
    "                grad_update = grad_accumulator / weight_accumulator\n",
    "                new_Phi[cluster] -= step_size * learning_rate * grad_update  # Small gradient update\n",
    "\n",
    "        # Step 3: Update priors dynamically based on shared information\n",
    "        # Each client independently updates its priors based on neighbors' contributions\n",
    "        for i in range(n):\n",
    "            neighbors = np.argsort(Pi[i])[-min(neighbors_per_iteration, clusters):]\n",
    "            weighted_sum = np.sum(W[i, neighbors])\n",
    "            if weighted_sum > 0:\n",
    "                Pi[i, neighbors] = W[i, neighbors] / weighted_sum  # Normalize to maintain valid distribution\n",
    "\n",
    "        # Replace Phi with new_Phi after all updates\n",
    "        Phi = new_Phi\n",
    "\n",
    "    return Phi, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9d5c5a-b100-4e7b-ae70-05c8ff708552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gibbs Sampling\n",
    "def personalized_gibbs_sampling(X, y, clusters, iterations=1000, burn_in=100, candidate_count=20):\n",
    "    n = len(X)\n",
    "    d = X[0].shape[1]\n",
    "    w_samples = np.zeros((iterations - burn_in, clusters, d))\n",
    "    w_current = np.random.randn(clusters, d)\n",
    "\n",
    "    for it in range(iterations):\n",
    "        for c in range(clusters):\n",
    "            # Choose a subset of candidates excluding data generator 1 (similar to personalized learning)\n",
    "            candidates = np.random.choice(range(1, n), size=candidate_count, replace=False)\n",
    "\n",
    "            # Compute weighted sum and total weight from candidates for updating the cluster parameter\n",
    "            weighted_sum = np.zeros(d)\n",
    "            total_weight = np.zeros((d, d))\n",
    "\n",
    "            # Iterate over the selected candidates\n",
    "            for i in candidates:\n",
    "                weighted_sum += X[i].T @ y[i]\n",
    "                total_weight += X[i].T @ X[i]\n",
    "\n",
    "            # Add the contribution from data generator 1 to personalize the model\n",
    "            weighted_sum += X[0].T @ y[0]\n",
    "            total_weight += X[0].T @ X[0]\n",
    "\n",
    "            # Sample new parameters from the conditional posterior distribution\n",
    "            w_current[c] = np.random.multivariate_normal(np.linalg.inv(total_weight) @ weighted_sum, np.linalg.inv(total_weight))\n",
    "\n",
    "        # Store the samples after burn-in period\n",
    "        if it >= burn_in:\n",
    "            w_samples[it - burn_in] = w_current\n",
    "\n",
    "    # Return the mean of the samples as the personalized estimate for data generator 1\n",
    "    return w_samples.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7857823c-b58d-42e3-98ff-71e84c8474ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance evaluation\n",
    "def evaluate_performance(w_true, w_est):\n",
    "    clusters = w_est.shape[0]\n",
    "    return np.mean([np.linalg.norm(w_est[i % clusters] - w_true[i]) ** 2 for i in range(len(w_true))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b27cf70-bb2d-4953-8a0e-4cfbde8b33e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Federated Learning Performance (MSE): 50.25453628175528\n",
      "Federated EM Performance (MSE): 22.6053920037701\n",
      "Gibbs Sampling Performance (MSE): 25.1067633407318\n",
      "\n",
      "============================\n",
      "\n",
      "Personalized Federated Learning Performance (MSE): 55.90809428111086\n",
      "Federated EM Performance (MSE): 50.12128159463204\n",
      "Gibbs Sampling Performance (MSE): 27.710867253945075\n",
      "\n",
      "============================\n",
      "\n",
      "Personalized Federated Learning Performance (MSE): 29.50282810503096\n",
      "Federated EM Performance (MSE): 35.597795718631566\n",
      "Gibbs Sampling Performance (MSE): 14.498815964838201\n",
      "\n",
      "============================\n",
      "\n",
      "Personalized Federated Learning Performance (MSE): 39.16617174200603\n",
      "Federated EM Performance (MSE): 20.306525336052726\n",
      "Gibbs Sampling Performance (MSE): 20.090412194319995\n",
      "\n",
      "============================\n",
      "\n",
      "Personalized Federated Learning Performance (MSE): 48.18541941843612\n",
      "Federated EM Performance (MSE): 20.363900931500975\n",
      "Gibbs Sampling Performance (MSE): 24.74855252937033\n",
      "\n",
      "============================\n",
      "\n",
      "Personalized Federated Learning Performance (MSE): 22.97202249748333\n",
      "Federated EM Performance (MSE): 25.301980649923834\n",
      "Gibbs Sampling Performance (MSE): 11.453539309386656\n",
      "\n",
      "============================\n",
      "\n",
      "Personalized Federated Learning Performance (MSE): 58.42020578450161\n",
      "Federated EM Performance (MSE): 40.79920425602744\n",
      "Gibbs Sampling Performance (MSE): 28.77958992434702\n",
      "\n",
      "============================\n",
      "\n",
      "Personalized Federated Learning Performance (MSE): 20.929061788356904\n",
      "Federated EM Performance (MSE): 42.39092324880813\n",
      "Gibbs Sampling Performance (MSE): 10.560700384645692\n",
      "\n",
      "============================\n",
      "\n",
      "Personalized Federated Learning Performance (MSE): 63.8936299493813\n",
      "Federated EM Performance (MSE): 59.185941363743915\n",
      "Gibbs Sampling Performance (MSE): 31.698988575627062\n",
      "\n",
      "============================\n",
      "\n",
      "Personalized Federated Learning Performance (MSE): 15.644767776424347\n",
      "Federated EM Performance (MSE): 39.31736261367722\n",
      "Gibbs Sampling Performance (MSE): 7.8945474163695515\n",
      "\n",
      "============================\n",
      "\n",
      "Average PFL Performance (MSE): 40.48767376244867\n",
      "Average EM Performance (MSE): 35.599030771676794\n",
      "Average Gibbs Sampling Performance (MSE): 20.25427768935814\n"
     ]
    }
   ],
   "source": [
    "n, m, d, clusters = 100, 10, 5, 2\n",
    "noise_var = 0.1\n",
    "iterations = 10\n",
    "\n",
    "w_pfl_avg = 0\n",
    "w_em_avg = 0\n",
    "w_gibbs_avg = 0\n",
    "\n",
    "for _ in range(iterations):\n",
    "    X, y, w_true = generate_data(n, m, d, clusters, noise_var)\n",
    "\n",
    "    # Run Personalized Federated Learning\n",
    "    w_pfl = personalized_federated_learning(X, y)\n",
    "\n",
    "    # Run Updated Expectation-Maximization for Federated Learning with Discrete Latent Assignment (`zi`)\n",
    "    w_em_federated, zi = personalized_expectation_maximization(X, y, clusters)\n",
    "\n",
    "    # Run Gibbs Sampling\n",
    "    w_gibbs = personalized_gibbs_sampling(X, y, clusters)\n",
    "\n",
    "    # Evaluate the performance of each method\n",
    "    pfl_performance = evaluate_performance(w_true, np.array([w_pfl]))\n",
    "    em_federated_performance = evaluate_performance(w_true, w_em_federated)\n",
    "    gibbs_performance = evaluate_performance(w_true, w_gibbs)\n",
    "    \n",
    "    w_pfl_avg += pfl_performance\n",
    "    w_em_avg += em_federated_performance\n",
    "    w_gibbs_avg += gibbs_performance\n",
    "\n",
    "    # Display performance results\n",
    "    print(f\"Personalized Federated Learning Performance (MSE): {pfl_performance}\")\n",
    "    print(f\"Federated EM Performance (MSE): {em_federated_performance}\")\n",
    "    print(f\"Gibbs Sampling Performance (MSE): {gibbs_performance}\")\n",
    "    \n",
    "    print(f\"\\n============================\\n\")\n",
    "    \n",
    "\n",
    "print(f\"Average PFL Performance (MSE): {w_pfl_avg / 10}\")\n",
    "print(f\"Average EM Performance (MSE): {w_em_avg / 10}\")\n",
    "print(f\"Average Gibbs Sampling Performance (MSE): {w_gibbs_avg / 10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef917ceb-bf55-40a5-846a-5a3c64853560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
